{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSnIdo1IihDQ",
        "colab_type": "code",
        "outputId": "58d4c737-227e-4ba3-f95e-52bfefcaed74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "!git clone https://github.com/hanearl/bert-for-tf2.git\n",
        "!pip install -r bert-for-tf2/requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert-for-tf2' already exists and is not an empty directory.\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (from -r bert-for-tf2/requirements.txt (line 1)) (0.14.4)\n",
            "Collecting focal-loss\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a8/2fcf3420d28754b7df2ddb0e06f44bcae66ad6c18a8dea12268c1d52f210/focal_loss-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2->-r bert-for-tf2/requirements.txt (line 1)) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2->-r bert-for-tf2/requirements.txt (line 1)) (0.9.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.6/dist-packages (from focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2->-r bert-for-tf2/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2->-r bert-for-tf2/requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.29.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2->focal-loss->-r bert-for-tf2/requirements.txt (line 2)) (0.4.8)\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tUNs8cWt6V0",
        "colab_type": "code",
        "outputId": "e29bd916-4821-4d32-a4cb-edf1cf7cd44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  6 13:17:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVy5wm1nqa-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pickle\n",
        "import math\n",
        "import json\n",
        "\n",
        "sys.path.append('bert-for-tf2')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer\n",
        "\n",
        "from custom_metrics import MultiLabelAccuracy\n",
        "from sentiments_data import SentimentsData\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZhagtyHtkCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./bert-for-tf2/config.json', 'r') as f:\n",
        "    config = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jArEIT0fnG9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_path = config[\"drive_path\"]\n",
        "train_name = config[\"train_name\"]\n",
        "\n",
        "project_path = os.path.join(drive_path, \"bert_sentiment\")\n",
        "bert_model_path = os.path.join(project_path, \"bert_model\")\n",
        "data_path = os.path.join(project_path, \"data\")\n",
        "epoch_log_path = os.path.join(project_path, \"epoch_logs\", train_name)\n",
        "epoch_model_path = os.path.join(project_path, \"epoch_models\", train_name)\n",
        "tb_path = os.path.join(project_path, \"logs\", train_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFS_YX442s3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(epoch_log_path):\n",
        "    os.mkdir(epoch_log_path)\n",
        "\n",
        "if not os.path.isdir(epoch_model_path):\n",
        "    os.mkdir(epoch_model_path)\n",
        "\n",
        "if not os.path.isdir(tb_path):\n",
        "    os.mkdir(tb_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iisF3z_aiu4y",
        "colab_type": "code",
        "outputId": "3b2bc53e-685e-42f1-a3a1-90ee33db2897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_name = config['model_name']\n",
        "model_dir = bert.fetch_google_bert_model(model_name, \".model\")\n",
        "model_ckpt = os.path.join(bert_model_path, model_dir, \"bert_model.ckpt\")\n",
        "\n",
        "# Tokenize\n",
        "do_lower_case = not (model_name.find(\"cased\") == 0 or model_name.find(\"multi_cased\") == 0)\n",
        "bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\n",
        "vocab_file = os.path.join(model_dir, \"vocab.txt\")\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "# bert ckpt path\n",
        "bert_ckpt_dir = os.path.join(bert_model_path, \"multi_cased_L-12_H-768_A-12\")\n",
        "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already  fetched:  multi_cased_L-12_H-768_A-12.zip\n",
            "already unpacked at: .model/multi_cased_L-12_H-768_A-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnxMdCajBYU",
        "colab_type": "code",
        "outputId": "c9377384-5622-4030-a7d9-4c32a7bebb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "with open(os.path.join(data_path, \"sentiments.pkl\"), \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "df = pd.read_csv(os.path.join(data_path, 'sentiments.csv'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (20,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-tYj1cOxwus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_layers(root_layer):\n",
        "    if isinstance(root_layer, keras.layers.Layer):\n",
        "        yield root_layer\n",
        "    for layer in root_layer._layers:\n",
        "        for sub_layer in flatten_layers(layer):\n",
        "            yield sub_layer\n",
        "\n",
        "\n",
        "def freeze_bert_layers(l_bert):\n",
        "    \"\"\"\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
        "    \"\"\"\n",
        "    for layer in flatten_layers(l_bert):\n",
        "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
        "            layer.trainable = True\n",
        "        elif len(layer._layers) == 0:\n",
        "            layer.trainable = False\n",
        "        l_bert.embeddings_layer.trainable = False\n",
        "\n",
        "\n",
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpI9p3yLjSAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.pred_sentences = [df.sentence[i] for i in range(10, 20)]\n",
        "        self.pred_sentiments = [df.sentiments[i] for i in range(10, 20)]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "    #def on_batch_end(self, epoch, logs=None):\n",
        "        epoch = epoch + 1\n",
        "        if epoch % config['save_model_period'] == 0:\n",
        "            self.model.save_weights(os.path.join(epoch_model_path, 'sentiments.h5'), overwrite=True)\n",
        "        pred_sentences = self.pred_sentences\n",
        "        pred_tokens = map(tokenizer.tokenize, pred_sentences)\n",
        "        pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
        "        pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
        "\n",
        "        pred_token_ids = map(lambda tids: tids + [0] * (128 - len(tids)), pred_token_ids)\n",
        "        pred_token_ids = np.array(list(pred_token_ids))\n",
        "\n",
        "        res = self.model.predict(pred_token_ids)\n",
        "        res = tf.sigmoid(res)\n",
        "        res = tf.cast(res > 0.5, dtype=tf.int32).numpy()\n",
        "\n",
        "        res_string = ''\n",
        "        res_string += 'epoch: {}\\n'.format(epoch)\n",
        "        for text, label, sentiment in zip(pred_sentences, self.pred_sentiments, res):\n",
        "            pred_sentiments = [data.code_to_senti[s-1] for s in sentiment * np.arange(1, 35) if s != 0]\n",
        "            res_string += \"text: {}\\nlabels: {}\\nres: {}\\n\\n\".format(text, label, pred_sentiments)\n",
        "\n",
        "        with open(os.path.join(epoch_log_path, 'epoch_res_{}.txt'.format(epoch)), 'w') as f:\n",
        "            f.write(res_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgHsZ9LQjljZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = os.path.join(tb_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p-39Jr7jnL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from focal_loss import BinaryFocalLoss\n",
        "\n",
        "def create_model(max_seq_len, adapter_size=64):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "    # adapter_size = 64  # see - arXiv:1902.00751\n",
        "\n",
        "    # create the bert layer\n",
        "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
        "        bc = StockBertConfig.from_json_string(reader.read())\n",
        "        bert_params = map_stock_config_to_params(bc)\n",
        "        bert_params.adapter_size = adapter_size\n",
        "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "\n",
        "    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
        "    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
        "    # output         = bert([input_ids, token_type_ids])\n",
        "    output = bert(input_ids)\n",
        "\n",
        "    print(\"bert shape\", output.shape)\n",
        "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
        "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "    logits = keras.layers.Dropout(0.5)(logits)\n",
        "    logits = keras.layers.Dense(units=34)(logits)\n",
        "\n",
        "    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
        "    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "    model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "    # load the pre-trained model weights\n",
        "    load_stock_weights(bert, bert_ckpt_file)\n",
        "\n",
        "    # freeze weights if adapter-BERT is used\n",
        "    if adapter_size is not None:\n",
        "        freeze_bert_layers(bert)\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid_cross_entropy_loss(true, pred):\n",
        "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=true)\n",
        "        loss = tf.reduce_mean(tf.reduce_sum(loss))\n",
        "        return loss\n",
        "    focal_loss = BinaryFocalLoss(gamma=1, from_logits=True)\n",
        "\n",
        "    loss_func_list = {\n",
        "        \"sigmoid_cross_entropy_loss\": sigmoid_cross_entropy_loss,\n",
        "        \"focal_loss\": focal_loss\n",
        "    }\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(),\n",
        "                  loss=loss_func_list[config['loss_func']],\n",
        "                  metrics=[MultiLabelAccuracy()])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNq-dhYmwwco",
        "colab_type": "code",
        "outputId": "e9b3e356-14af-48db-e7c4-24a0c9c4fa52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "adapter_size = None # use None to fine-tune all of BERT\n",
        "model = create_model(data.max_seq_len, adapter_size=adapter_size)\n",
        "total_epoch_count = config['num_epochs']\n",
        "\n",
        "model.fit(x=data.train_x, y=data.train_y,\n",
        "          validation_split=0.1,\n",
        "          batch_size=config['batch_size'],\n",
        "          shuffle=True,\n",
        "          epochs=total_epoch_count,\n",
        "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
        "                                                    end_learn_rate=1e-7,\n",
        "                                                    warmup_epoch_count=config['warmup_epoch_count'],\n",
        "                                                    total_epoch_count=total_epoch_count),\n",
        "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
        "\n",
        "                     tensorboard_callback, MyCustomCallback()])\n",
        "model.save_weights(os.path.join(epoch_model_path, 'sentiments_fin.h5'), overwrite=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 128, 768)\n",
            "Done loading 196 BERT weights from: /content/drive/My Drive/bert_sentiment/bert_model/multi_cased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fa7f7602978> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 128, 768)          177261312 \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 34)                26146     \n",
            "=================================================================\n",
            "Total params: 177,878,050\n",
            "Trainable params: 177,878,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nk-G3Jhyl7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adapter_size = None # use None to fine-tune all of BERT\n",
        "model = create_model(data.max_seq_len, adapter_size=adapter_size)\n",
        "\n",
        "model.load_weights(os.path.join(epoch_model_path, 'sentiments_fin.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gU1mCpy_AZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_sentences = [df.sentence[i] for i in range(10230, 10270)]\n",
        "# pred_sentiments = [df.sentiments[i] for i in range(100, 150)]\n",
        "# pred_sentences = [\n",
        "#                   \"공감을 누르지말아달라하시니 눌러드리는게 인지상정!\",\n",
        "#                   \"저도.. 아무리 장난이라지만 다른 사람의 마음을 가지고 장난칠 수 있는 이런걸 보고 웃는게 별로네요. 장난인데 제가 너무 심각하게 받아들인건지는 몰라도..\",\n",
        "#                   \"선의가 아닌 오자랖. 자기차는 그러고 있으면 사고 안나나?\",\n",
        "#                   \"손크기 차이 뭐야ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ뭐냐고ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ언니 너무 커여워요ㅠㅠㅠㅠㅠㅠㅠㅠ 언니 주먹으로 꿀밤맞고싶다ㅠㅠㅠㅠ 귀여워ㅠㅠㅠㅠㅠㅠ\",\n",
        "#                   \"나라카일 !! 개리형은 다이아에 있을 실력이 아니야 카일이 형이랑 같이 올라가자!\",\n",
        "#                   \"진짜 나라카일이랑 해도 이기기가 힘드네ㅋㅋ 킹텀ㅋㅋ 진짜ㅋㅋ\",\n",
        "#                   \"몽자야.. 너 강아지 아니지 사람이잖아.. 그치..?\",\n",
        "#                   \"몽자 댕청해ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ몸은 김종국인데 왤케 뽀짝하지\",\n",
        "#                   \"몽자도 몽자인데 주인 부부님이 너무 유쾌하고 웃기셔 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ ㅋㅋㅋㅋㅋ\",\n",
        "#                   \"몽자는 귓털이 횟가닥 깻잎머리마냥 앞 얼굴에 사방팔방 뒤엉켜 붙을때 잴 귀여움\",\n",
        "#                   \"몽자야ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ너무기여우어ㅠㅠㅜㅜㅜㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ 고개 갸우뚱~?\",\n",
        "#                   \"우리 몽자 너무너무 사랑해ㅐ\",\n",
        "#                   \"몽자야ㅠㅠㅠㅠㅠ 머리 부스스한 거 왜 이렇게 귀여운 거야ㅠㅠㅠㅠ 승질내도 왜 귀엽기만 한 거야 ㅠㅠㅠㅠ\",\n",
        "#                   \"아니얘는 평소에도 부스스해서 너무 웃겨 ㅋㅌㅌㅌㅋㅋㅌㅌㅌㅋㅋ\",\n",
        "\n",
        "# ]\n",
        "pred_tokens = map(tokenizer.tokenize, pred_sentences)\n",
        "pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
        "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
        "\n",
        "pred_token_ids = map(lambda tids: tids + [0] * (128 - len(tids)), pred_token_ids)\n",
        "pred_token_ids = np.array(list(pred_token_ids))\n",
        "\n",
        "res = model.predict(pred_token_ids)\n",
        "# res = sig(res)\n",
        "res = tf.sigmoid(res)\n",
        "\n",
        "\n",
        "res = tf.cast(res > 0.5, dtype=tf.int32).numpy()\n",
        "res_string = ''\n",
        "for text, sentiment in zip(pred_sentences,res):\n",
        "    pred_sentiments = [data.code_to_senti[s-1] for s in sentiment * np.arange(1, 35) if s != 0]\n",
        "    # pred_sentiments.append(data.code_to_senti[max_sentiments[0]])\n",
        "    # res_string += \"text: {}\\nres: {}\\n\\n\".format(text, pred_sentiments)\n",
        "    # x = [(idx, prop) for idx, prop in enumerate(sentiment)]\n",
        "    # x = sorted(x, key=lambda x: x[1], reverse=True)\n",
        "    # pred_sentiments = [(data.code_to_senti[s[0]], s[1].numpy()*100) for s in x][:3]\n",
        "    res_string += \"text: {}\\nres: {}\\n\\n\".format(text, pred_sentiments)\n",
        "\n",
        "print(res_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9lCC-6B_Ty2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku5Xxq49C5ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sig(output):\n",
        "    output = tf.math.log(output / (1 - output))\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwUfsEknDAX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}